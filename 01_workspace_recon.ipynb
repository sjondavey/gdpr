{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of documents about GDPR. I need\n",
    "1) A place for the original (pdf / web text etc)\n",
    "2) A notebook to \n",
    "    a) convert the original into a dataframe; \n",
    "    b) to create summaries and questions that can be added to the document index (QUESTION: is this one file / table or one per document?) \n",
    "   There notebooks are to be saved in folder ./conversion_notebooks/\n",
    "3) A document.py wrapper for the dataframe version of the document ./gdpr_rag/documents/\n",
    "4) A naming convention that that allows a script to check that each original document has been converted into a dataframe, has a document.py wrapper and has been added to the document index. This should also check that there are no additional entries in the document index etc.\n",
    "\n",
    "\n",
    "This workbook does the recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_documentation = \"./original/\"\n",
    "df_version_of_document = \"./inputs/documents/\"\n",
    "index_for_document = \"./inputs/index/\"\n",
    "python_wrappers = \"./gdpr_rag/documents/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'article_30_5': 'Article_30_5', 'article_47_bcr': 'Article_47_BCR', 'article_49_intl_transfer': 'Article_49_Intl_Transfer', 'codes': 'Codes', 'consent': 'Consent', 'covid_health': 'CovidHealth', 'covid_location': 'CovidLocation', 'data_breach': 'DataBreach', 'data_portability': 'DataPortability', 'decision_making': 'DecisionMaking', 'dpia': 'DPIA', 'dpo': 'DPO', 'forgotten': 'Forgotten', 'gdpr': 'GDPR', 'lead_sa': 'Lead_SA', 'online_services': 'OnlineServices', 'protection': 'Protection', 'territorial_scope': 'TerritorialScope', 'transparency': 'Transparency', 'video': 'Video'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "\n",
    "def find_class_names_in_files(directory):\n",
    "    class_dict = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".py\"):  # Check for Python files\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, 'r') as file:\n",
    "                file_content = file.read()\n",
    "            tree = ast.parse(file_content)\n",
    "            for node in ast.walk(tree):\n",
    "                if isinstance(node, ast.ClassDef):\n",
    "                    class_name = node.name\n",
    "                    file_name_without_extension = os.path.splitext(filename)[0]\n",
    "                    class_dict[file_name_without_extension] = class_name\n",
    "                    break  # Assuming one class per file, break after finding the first class\n",
    "    return class_dict\n",
    "\n",
    "# Usage\n",
    "directory = python_wrappers  # Specify your folder path\n",
    "class_names_dict = find_class_names_in_files(directory)\n",
    "print(class_names_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "originals : {'video', 'online_services', 'territorial_scope', 'covid_health', 'lead_sa', 'article_30_5', 'data_portability', 'article_49_intl_transfer', 'transparency', 'dpo', 'covid_location', 'protection', 'codes', 'article_47_bcr', 'data_breach', 'gdpr', 'consent', 'forgotten', 'decision_making', 'dpia'}\n",
      "df_versions : {'video', 'online_services', 'territorial_scope', 'covid_health', 'lead_sa', 'article_30_5', 'data_portability', 'article_49_intl_transfer', 'transparency', 'dpo', 'covid_location', 'protection', 'codes', 'article_47_bcr', 'data_breach', 'gdpr', 'consent', 'forgotten', 'decision_making', 'dpia'}\n",
      "classes_in_df_version : {'OnlineServices', 'TerritorialScope', 'DPO', 'CovidLocation', 'Protection', 'GDPR', 'Video', 'DataPortability', 'Lead_SA', 'Article_49_Intl_Transfer', 'CovidHealth', 'Consent', 'DataBreach', 'Article_47_BCR', 'Forgotten', 'Transparency', 'Codes', 'Article_30_5', 'DPIA', 'DecisionMaking'}\n",
      "wrappers : {'video', 'online_services', 'territorial_scope', 'covid_health', 'lead_sa', 'article_30_5', 'data_portability', 'article_49_intl_transfer', 'transparency', 'dpo', 'covid_location', 'protection', 'codes', 'article_47_bcr', 'data_breach', 'gdpr', 'consent', 'forgotten', 'decision_making', 'dpia'}\n",
      "indexes : {'video', 'online_services', 'territorial_scope', 'covid_health', 'lead_sa', 'article_30_5', 'data_portability', 'article_49_intl_transfer', 'transparency', 'dpo', 'covid_location', 'protection', 'codes', 'article_47_bcr', 'data_breach', 'gdpr', 'consent', 'forgotten', 'decision_making', 'dpia'}\n",
      "names_of_documents_in_index_data : {'OnlineServices', 'TerritorialScope', 'CovidLocation', 'DPO', 'Protection', 'GDPR', 'Video', 'DataPortability', 'Lead_SA', 'Article_49_Intl_Transfer', 'CovidHealth', 'Consent', 'DataBreach', 'Article_47_BCR', 'Forgotten', 'Transparency', 'Codes', 'Article_30_5', 'DPIA', 'DecisionMaking'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def extract_root_names(folder_path):\n",
    "    root_names = [os.path.splitext(file)[0] for file in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, file))]\n",
    "    return root_names\n",
    "\n",
    "originals = set(extract_root_names(original_documentation))\n",
    "df_versions = set(extract_root_names(df_version_of_document))\n",
    "classes_in_df_version = {class_names_dict[df_version_name] for df_version_name in df_versions}\n",
    "\n",
    "indexes = set(extract_root_names(df_version_of_document))\n",
    "wrappers = set(extract_root_names(python_wrappers))\n",
    "\n",
    "names_of_documents_in_index_data = []\n",
    "import os\n",
    "from regulations_rag.standard_regulation_index import load_parquet_data, save_parquet_data\n",
    "key = os.getenv('encryption_key_gdpr')\n",
    "for file in os.listdir(index_for_document):\n",
    "    df = load_parquet_data(os.path.join(index_for_document, file), key)\n",
    "    documents_referenced = set(df[\"document\"].to_list())\n",
    "    # if not documents_referenced.issubset(originals):\n",
    "    #     not_in_originals = documents_referenced.difference(originals)\n",
    "    #     print(f\"The file {file} contains a reference to a document that does not exist. The problem reference(s) in the dataframe index are {not_in_originals}\")\n",
    "    \n",
    "    names_of_documents_in_index_data = names_of_documents_in_index_data + list(documents_referenced)\n",
    "\n",
    "names_of_documents_in_index_data = set(names_of_documents_in_index_data)\n",
    "\n",
    "print_all = True\n",
    "if print_all:\n",
    "    print(f\"originals : {originals}\")\n",
    "    print(f\"df_versions : {df_versions}\")\n",
    "    print(f\"classes_in_df_version : {classes_in_df_version}\")\n",
    "    print(f\"wrappers : {wrappers}\")\n",
    "    print(f\"indexes : {indexes}\")\n",
    "    print(f\"names_of_documents_in_index_data : {names_of_documents_in_index_data}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- originals and df_versions match\n",
      "- originals and wrappers match\n",
      "- originals and indexes match\n",
      "- class names in python documents and index database match\n"
     ]
    }
   ],
   "source": [
    "only_in_originals = originals - df_versions\n",
    "not_in_originals = df_versions - originals\n",
    "\n",
    "if only_in_originals:\n",
    "    print(f\"Items in original that are not in df_versions: {only_in_originals}\")\n",
    "elif not_in_originals:\n",
    "    print(f\"Items in df_versions that are not in original: {not_in_originals}\")\n",
    "else:\n",
    "    print(\"- originals and df_versions match\")\n",
    "\n",
    "only_in_originals = originals - wrappers\n",
    "not_in_originals = wrappers - originals\n",
    "\n",
    "if only_in_originals:\n",
    "    print(f\"Items in original that are not in wrapper: {only_in_originals}\")\n",
    "elif not_in_originals:\n",
    "    print(f\"Items in wrapper that are not in original: {not_in_originals}\")\n",
    "else:\n",
    "    print(\"- originals and wrappers match\")\n",
    "\n",
    "only_in_originals = originals - indexes\n",
    "not_in_originals = indexes - originals\n",
    "\n",
    "if only_in_originals:\n",
    "    print(f\"Items in original that are not indexed: {only_in_originals}\")\n",
    "elif not_in_originals:\n",
    "    print(f\"Items indexed that are not in original: {not_in_originals}\")\n",
    "else:\n",
    "    print(\"- originals and indexes match\")\n",
    "\n",
    "classes_only_in_files = classes_in_df_version - names_of_documents_in_index_data\n",
    "names_only_in_index = names_of_documents_in_index_data - classes_in_df_version\n",
    "\n",
    "if classes_only_in_files:\n",
    "    print(f\"Class names in python documents that are not in names_of_documents_in_index_data: {classes_only_in_files}\")\n",
    "elif names_only_in_index:\n",
    "    print(f\"Index classes that do not have python document wrappers: {not_in_originals}\")\n",
    "else:\n",
    "    print(\"- class names in python documents and index database match\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now import the class into gdpr_corpus.rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path= '../gdpr/gdpr_rag/gdpr_corpus.py'\n",
    "\n",
    "def add_import_to_gdpr_corpus(new_py_file_name, class_name, file_path=file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    # Find the last import statement\n",
    "    last_import_index = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.startswith('from gdpr_rag.documents'):\n",
    "            last_import_index = i\n",
    "\n",
    "    # Create the new import statement\n",
    "    new_import = f\"from gdpr_rag.documents.{new_py_file_name} import {class_name}\\n\"\n",
    "    \n",
    "    # Insert the new import statement after the last existing import\n",
    "    lines.insert(last_import_index + 1, new_import)\n",
    "\n",
    "    # Write the updated lines back to the file\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.writelines(lines)\n",
    "\n",
    "# Usage\n",
    "add_import_to_gdpr_corpus('protection', 'Protection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20591bc9273590117cdd0f52559c248ef39f0181da66e3521068e03aa47654cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
