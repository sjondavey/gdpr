{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "log_level = 25\n",
    "logging.basicConfig(level=log_level) # root logger\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('E:/Code/chat/gdpr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 11b62271-8ec0-4088-88ee-1138b2f81c01\n",
      "........."
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "from llama_parse import LlamaParse  # pip install llama-parse\n",
    "import os\n",
    "llamaindex_key = os.getenv('llamaindex')\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "parser = LlamaParse(\n",
    "    api_key=llamaindex_key, \n",
    "    result_type=\"markdown\"  # \"markdown\" and \"text\" are available\n",
    ")\n",
    "\n",
    "documents = parser.load_data(\"E:/Code/chat/gdpr/pdf/guidelines/wp251rev_01_en_A754F3E1-FB46-9E76-C0A919864E4B6641_49826.pdf\") \n",
    "\n",
    "file = \"../tmp/decision_making_llama.md\"\n",
    "with open(file, 'w', encoding='utf-8') as f:\n",
    "    f.write(documents[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "import os\n",
    "path = 'E:/Code/chat/gdpr/pdf/guidelines'\n",
    "file_name = 'wp251rev_01_en_A754F3E1-FB46-9E76-C0A919864E4B6641_49826.pdf'\n",
    "paper_name = 'Guidelines on Automated individual decision-making and Profiling for the purposes of Regulation 2016/679'\n",
    "full_path = os.path.join(path, file_name)\n",
    "doc = fitz.open(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import src.extract_from_pdf\n",
    "importlib.reload(src.extract_from_pdf)\n",
    "from src.extract_from_pdf import output_doc_as_text\n",
    "\n",
    "lines_to_delete = [] \n",
    "characters_to_replace = []\n",
    "characters_to_replace.append(['“', '\"'])\n",
    "characters_to_replace.append(['”', '\"'])\n",
    "characters_to_replace.append(['‘', \"'\"])\n",
    "characters_to_replace.append(['’', \"'\"])\n",
    "characters_to_replace.append(['', \"-\"])\n",
    "characters_to_replace.append(['–', \"-\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "document_text = output_doc_as_text(doc, start_page = 4, end_page = 0, header_size=70, footer_size=90, lines_to_delete = lines_to_delete, characters_to_replace = characters_to_replace)\n",
    "\n",
    "file = \"../tmp/decision_making.txt\"\n",
    "with open(file, 'w', encoding='utf-8') as f:\n",
    "    f.write(document_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once you have the markdown file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "file_path = \"../../original/decision_making.md\"\n",
    "with open(file_path, 'r', encoding = \"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "lines = text.split('\\n')\n",
    "# get rid of empty lines\n",
    "lines = [line for line in lines if line]\n",
    "\n",
    "doc_as_array = []\n",
    "notes_as_array = []\n",
    "#footnote_pattern = re.compile(r'^(\\[\\^\\d{1,2}\\]:)(.*)$')\n",
    "footnote_pattern = re.compile(r'^\\[\\^(\\d{1,2})\\]:(.*)$')\n",
    "for entry in lines:\n",
    "    footnote_match = footnote_pattern.match(entry)\n",
    "    if footnote_match:\n",
    "        notes_as_array.append([footnote_match.group(1), footnote_match.group(2).strip()])\n",
    "    else:\n",
    "        doc_as_array.append(entry)\n",
    "\n",
    "columns = [\"note_number\", \"text\"]\n",
    "df_notes = pd.DataFrame(notes_as_array, columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"section\", \"subsection\", \"point\", \"heading\", \"text\", \"section_reference\"]\n",
    "section = \"\"\n",
    "subsection = \"\"\n",
    "point = \"\"\n",
    "heading = False\n",
    "text = \"\"\n",
    "section_reference = \"\"\n",
    "section_pattern = re.compile(r'\\b(I|II|III|IV|V|VI)\\.\\s*(.*)')\n",
    "subsection_pattern = re.compile(r'([A-Z])\\.\\s*(.*)') \n",
    "point_pattern = re.compile(r'(\\d+)\\.\\s*(.*)$') \n",
    "annex_pattern = re.compile(r'^ANNEX (\\d+) - (.+)$') # ANNEX, number,  - ,  text\n",
    "\n",
    "table_pattern = re.compile(r'\\|')\n",
    "# table_entry_number_pattern = re.compile(r'(\\d+(\\.\\d+)?(\\.\\d+)?)\\s+(.+)')\n",
    "# table_section = re.compile(r'<td colspan=6>(\\d+) - (.+)$')\n",
    "# table_subsection = re.compile(r'<td colspan=6>\\s*(.+)\\s*')\n",
    "\n",
    "\n",
    "data = []\n",
    "table_data = []\n",
    "for entry in doc_as_array:\n",
    "    section_match = section_pattern.match(entry)\n",
    "    subsection_match = subsection_pattern.match(entry)\n",
    "    point_match = point_pattern.match(entry)\n",
    "    annex_match = annex_pattern.match(entry)\n",
    "    table_match = table_pattern.match(entry)\n",
    "    if section_match:\n",
    "        match = section_match\n",
    "        section = match.group(1)\n",
    "        subsection = \"\"\n",
    "        point = \"\"\n",
    "        heading = True\n",
    "        text = match.group(2)\n",
    "        section_reference = section\n",
    "        data.append([section, subsection, point, heading, text, section_reference])\n",
    "    elif subsection_match:\n",
    "        match = subsection_match\n",
    "        section = section\n",
    "        subsection = match.group(1)\n",
    "        point = \"\"\n",
    "        heading = True\n",
    "        text = match.group(2)\n",
    "        section_reference = section + \".\" + subsection\n",
    "        data.append([section, subsection, point, heading, text, section_reference])\n",
    "    elif point_match:\n",
    "        match = point_match\n",
    "        section = section\n",
    "        subsection = subsection\n",
    "        point = match.group(1)\n",
    "        heading = True\n",
    "        text = match.group(2)\n",
    "        section_reference = section + \".\" + subsection + \".\" + point\n",
    "        data.append([section, subsection, point, heading, text, section_reference])\n",
    "    elif annex_match:\n",
    "        match = annex_match\n",
    "        section = \"Annex \" + match.group(1)\n",
    "        subsection = \"\"\n",
    "        point = \"\"\n",
    "        heading = True\n",
    "        text = match.group(2)\n",
    "        section_reference = section\n",
    "        data.append([section, subsection, point, heading, text, section_reference])\n",
    "    # elif table_match:\n",
    "    #     pass\n",
    "        # components = [component.strip() for component in entry.strip().strip('|').split('|')]\n",
    "        # if len(components) == 5:\n",
    "        #     table_entry_number_match = table_entry_number_pattern.match(components[0])\n",
    "        #     if table_entry_number_match:\n",
    "        #         components = [table_entry_number_match.group(1), table_entry_number_match.group(4)] + components[1:]\n",
    "        #         table_data.append(components)\n",
    "        #     else:\n",
    "        #         print(f\"Row removed from table_data: {components}\")\n",
    "        #         #components = [\"\"] + components\n",
    "        # elif len(components) == 1:\n",
    "        #     table_section_match = table_section.match(components[0])\n",
    "        #     table_subsection_match = table_subsection.match(components[0])\n",
    "        #     if table_section_match:\n",
    "        #         components = [table_section_match.group(1), table_section_match.group(2)]\n",
    "        #         table_data.append(components)\n",
    "        #     elif table_subsection_match:\n",
    "        #         print(f\"Row removed from table_data: {components}\")\n",
    "        #         # components = [\"\", table_subsection_match.group(1)]\n",
    "        #     else:\n",
    "        #         raise AttributeError(f\"A row with one element did not match the required patterns: {entry}\")\n",
    "        # else:\n",
    "        #     raise AttributeError(\"A table entry did not have the correct number of elements\")\n",
    "    else:\n",
    "        section = section\n",
    "        subsection = subsection\n",
    "        point = point\n",
    "        heading = False\n",
    "        text = entry\n",
    "        section_reference = section\n",
    "        if subsection:\n",
    "            if point:\n",
    "                section_reference = section + \".\" + subsection + \".\" + point\n",
    "            else:\n",
    "                section_reference = section + \".\" + subsection\n",
    "        \n",
    "        data.append([\"\", \"\", \"\", heading, text, section_reference])\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data, columns = columns)\n",
    "# Remove my note about the table\n",
    "#df = df[df[\"text\"] != 'Note this table contains a column \"References to BCR-C, application form BCR-C, and / or supporting documents[^14]\" which is empty in the document because it is supposed to be filled out by the controller'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_no_table = df[~df[\"section_reference\"].str.contains(\"Annex\")]\n",
    "df_no_table = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "section                                          \n",
      "subsection                                       \n",
      "point                                            \n",
      "heading                                     False\n",
      "text                 |Article| Recital| Comments|\n",
      "section_reference                         Annex 2\n",
      "Name: 351, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_no_table.iloc[-50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 1 augmented with footnotes\n",
      "Row 40 augmented with footnotes\n",
      "Row 71 augmented with footnotes\n",
      "Row 75 augmented with footnotes\n",
      "Row 77 augmented with footnotes\n",
      "Row 79 augmented with footnotes\n",
      "Row 87 augmented with footnotes\n",
      "Row 91 augmented with footnotes\n",
      "Row 92 augmented with footnotes\n",
      "Row 113 augmented with footnotes\n",
      "Row 117 augmented with footnotes\n",
      "Row 119 augmented with footnotes\n",
      "Row 125 augmented with footnotes\n",
      "Row 130 augmented with footnotes\n",
      "Row 133 augmented with footnotes\n",
      "Row 136 augmented with footnotes\n",
      "Row 137 augmented with footnotes\n",
      "Row 143 augmented with footnotes\n",
      "Row 150 augmented with footnotes\n",
      "Row 156 augmented with footnotes\n",
      "Row 159 augmented with footnotes\n",
      "Row 161 augmented with footnotes\n",
      "Row 162 augmented with footnotes\n",
      "Row 166 augmented with footnotes\n",
      "Row 172 augmented with footnotes\n",
      "Row 180 augmented with footnotes\n",
      "Row 181 augmented with footnotes\n",
      "Row 189 augmented with footnotes\n",
      "Row 197 augmented with footnotes\n",
      "Row 200 augmented with footnotes\n",
      "Row 203 augmented with footnotes\n",
      "Row 252 augmented with footnotes\n",
      "Row 262 augmented with footnotes\n",
      "Row 270 augmented with footnotes\n",
      "Row 273 augmented with footnotes\n",
      "Row 279 augmented with footnotes\n",
      "Row 282 augmented with footnotes\n",
      "Row 286 augmented with footnotes\n",
      "Row 295 augmented with footnotes\n",
      "Row 312 augmented with footnotes\n",
      "Row 316 augmented with footnotes\n",
      "Row 322 augmented with footnotes\n",
      "Row 323 augmented with footnotes\n",
      "Row 325 augmented with footnotes\n",
      "Row 332 augmented with footnotes\n",
      "Row 337 augmented with footnotes\n",
      "Row 338 augmented with footnotes\n",
      "Row 340 augmented with footnotes\n",
      "Row 343 augmented with footnotes\n"
     ]
    }
   ],
   "source": [
    "# Add footnotes\n",
    "import re\n",
    "\n",
    "def find_footnote_references(text):\n",
    "    pattern = r'\\[\\^(\\d+)\\]'\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "for index, row in df_no_table.iterrows():\n",
    "    footnotes = find_footnote_references(row['text'])\n",
    "    if footnotes:\n",
    "        augmented_note = row['text']\n",
    "        for note in footnotes:\n",
    "            augmented_note += f\"\\n[^{note}]: {df_notes[df_notes['note_number'] == note].iloc[0]['text']}\"\n",
    "        print(f\"Row {index} augmented with footnotes\")\n",
    "        #print(augmented_note)\n",
    "        df_no_table.at[index, \"text\"] = augmented_note\n",
    "        #print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Article| Recital| Comments|\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[-50][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"../../inputs/documents/decision_making.parquet\" # use parquet to deal with the complex text so I don't need to worry about escape characters\n",
    "df.to_parquet(file, engine = 'pyarrow')\n",
    "\n",
    "#df_no_table.to_csv(file, encoding = \"utf-8\", sep=\"|\", index = False, na_rep=\"\", quotechar='\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that the document class works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('E:/Code/chat/gdpr')\n",
    "\n",
    "import importlib\n",
    "import gdpr_rag.documents.decision_making\n",
    "importlib.reload(gdpr_rag.documents.decision_making)\n",
    "from gdpr_rag.documents.decision_making import DecisionMaking\n",
    "\n",
    "path_to_manual_as_csv_file = \"../../inputs/documents/decision_making.parquet\"\n",
    "\n",
    "doc = DecisionMaking(path_to_manual_as_csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "III General provisions on profiling and automated decision-making\n",
      "III.B Lawful bases for processing\n",
      "III.B.6 Article 6(1) (f) - necessary for the legitimate interests[^18] pursued by the controller or by a third party\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# III General provisions on profiling and automated decision-making\n",
       "\n",
       "## III.B Lawful bases for processing\n",
       "\n",
       "### III.B.6 Article 6(1) (f) - necessary for the legitimate interests[^18] pursued by the controller or by a third party\n",
       "\n",
       "Profiling is allowed if it is necessary for the purposes of the legitimate interests[^19] pursued by the controller or by a third party. However, Article 6(1) (f) does not automatically apply just because the controller or third party has a legitimate interest. The controller must carry out a balancing exercise to assess whether their interests are overridden by the data subject's interests or fundamental rights and freedoms.\n",
       "\n",
       "The following are particularly relevant:\n",
       "\n",
       "- the level of detail of the profile (a data subject profiled within a broadly described cohort such as 'people with an interest in English literature', or segmented and targeted on a granular level);\n",
       "\n",
       "- the comprehensiveness of the profile (whether the profile only describes a small aspect of the data subject, or paints a more comprehensive picture);\n",
       "\n",
       "- the impact of the profiling (the effects on the data subject); and\n",
       "\n",
       "- the safeguards aimed at ensuring fairness, non-discrimination and accuracy in the profiling process.\n",
       "\n",
       "Although the WP29 opinion on legitimate interests[^20] is based on Article 7 of the data protection Directive 95/46/EC (the Directive), it contains examples that are still useful and relevant for controllers carrying out profiling. It also suggests it would be difficult for controllers to justify using legitimate interests as a lawful basis for intrusive profiling and tracking practices for marketing or advertising purposes, for example those that involve tracking individuals across multiple websites, locations, devices, services or  data-brokering.\n",
       "\n",
       "The controller should also consider the future use or combination of profiles when assessing the validity of processing under Article 6(1) (f).\n",
       "\n",
       "  \n",
       "[^18]: Legitimate interests listed in GDPR Recital 47 include processing for direct marketing purposes and processing strictly necessary for the purposes of preventing fraud.  \n",
       "[^19]: The controller's \"legitimate interest\" cannot render profiling lawful if the processing falls within the Article 22(1) definition.  \n",
       "[^20]: Article 29 Data Protection Working Party. Opinion 06/2014 on the notion of legitimate interests of the data controller under Article 7 of Directive 95/46/EC. European Commission, 9 April 2014, Page 47, examples on pages 59 and 60 http://ec.europa.eu/justice/data-protection/article-29/documentation/opinion- recommendation/files/2014/wp217_en.pdf . Accessed 24 April 2017"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "section = \"II\"\n",
    "section = \"I\"\n",
    "section = \"II.B\"\n",
    "section = \"II.C\"\n",
    "section = \"III.B.6\"\n",
    "#section = \"Annex 2\"\n",
    "\n",
    "# section = \"IV.E.1\"\n",
    "# section = \"III.A.1\"\n",
    "# section = \"III.D.4\"\n",
    "\n",
    "# section = \"IV\"\n",
    "# section = \"IV.E\"\n",
    "# section = \"IV.E.1\"\n",
    "\n",
    "print(doc.get_heading(section))\n",
    "display(Markdown(doc.get_text(section)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20591bc9273590117cdd0f52559c248ef39f0181da66e3521068e03aa47654cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
